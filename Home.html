<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css" integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous">

    <!--CSS Style Sheet -->
    <link rel="stylesheet" href="style.css">

    <title>Home</title>
  </head>



  <body>
    <header>
      <div id="CollegeLogo">
        <img src="https://www.doc.ic.ac.uk/~afd/homepages/images/logo_imperial_college_london.png" alt="CollegeLogo">
      </div>
      <div id="Collegename">Department of Computing</div>

    </header>

  <div id="Doctorname">
    Andrea Gadotti
  </div>

  <div id="PhDtitle">
    PhD student in Computational Privacy
  </div>


  <nav class="navbar sticky-top navbar-expand-lg navbar-light">
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNavDropdown">
      <ul class="navbar-nav">
        <li class="nav-item active">
          <a class="nav-link" href="Home.html">Home <span class="sr-only">(current)</span></a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#section01">Research Problem</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#section02">Different Solutions</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#section03">Field of Research</a>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Pioneers</a>
          <div class="dropdown-menu" id="dropdown">
            <a class="dropdown-item" href="#section04">Amazon</a>
            <a class="dropdown-item" href="#section04">Apple</a>
            <a class="dropdown-item" href="#section04">Google</a>
          </div>
        </li>
      </ul>
    </div>
  </nav>

<ul id="verticalnav">
  <div class="row">

    <div id="sideNav" class="list-group">
      <a class="list-group-item list-group-item-action" href="Home.html">Home</a>
      <a class="list-group-item list-group-item-action" href="#section01">Research Problem</a>
      <a class="list-group-item list-group-item-action" href="#section02">Different Solutions</a>
      <a class="list-group-item list-group-item-action" href="#section03">Field of Research</a>
      <a class="list-group-item list-group-item-action" href="#section04">Pioneers</a>
    </div>


    <div class="col-sm-12 col-lg-8" id="maintext" style="font-size: 2.5rem">
      Andrea received his MSc in mathematical logic from the University of Turin, and his major research field was set theory for his master’s degree. However, his current research speciality involves differential privacy, determining vulnerabilities in data-release systems proposed by others, and designing privacy-preserving mechanisms. His research topic is computational privacy, with which he, as well as his group, aims to have a direct and relatively immediate positive impact on the world while working as a scientist. Specific topics he is sensitive to include open sources, ethics in AI, general fairness and privacy of computing.
    </div>

    <div class="col-sm-12 col-lg-2" id="Phdimage">
      <img src="https://miro.medium.com/fit/c/240/240/1*uhKwuBtsy2r-QAiyNVbjxw.jpeg" alt="Phd Image">
    </div>

    <div id="mainsubheading">
      About Publication
    </div>
    <div id="publication">
The PhD student authored (alongside members of his <a href = "https://cpg.doc.ic.ac.uk/">research team</a>) a paper in which they presented a privacy attack on a “sticky noise” system called Diffix, developed by the startup Aircloak and researchers from the Max Planck Institute for Software Systems. Diffix’s proprietary approach adds static noise (random) and dynamic noise (based on the query and the query results).
They show how an attacker who knows attributes uniquely identifying a person in the dataset can figure out sensitive information about the person. The attack goes as follows:
</div>
<div id="bullet1">
1) Ask questions similar enough that the answers share the static noise, which can then be eliminated.
</div>
<div id="bullet2">
2) Run a likelihood ratio test to figure out the probability of the dynamic noise having different distributions (depending on the the value of the attribute the attacker wants to find).
</div>
<div id="bullet3">
3) Improve the accuracy of the method by designing other queries equivalent to those already done.
</div>
<div id="remaining">
The authors consider the main contribution to be the method of exploiting the noise added by Diffix. Considering the general academic intention is to develop a mathematically near-perfect solution for preserving privacy, many believe that differential privacy will not become viable (including Paul Francis, the co-founder of Aircloak, who calls it <a href="https://iapp.org/news/a/differential-privacy-at-the-end-of-the-rainbow/"> “the most over-hyped technology I have ever seen" </a>).
    </div>
  </div>
</ul>

<section id="section01" class="demo">
  <div id="heading1">
  Research Problem
  </div>
  <div id="text1">
  The definition of privacy is having control over your data. Although some people argue that privacy does not exist on the Internet, Andrea maintained that releasing some personal information is not equal to ignorance of privacy. While we only want to share a certain part of our personal information online, in general, we don’t want all of our data to be accessed by attackers. For example, if a message is sent between two friends, and the friends don’t want anyone else to read the message no matter what, the basic attack model is formed. In the model, the two friends are the users and anyone else who wants to retrieve the message are the attackers. Another possible concern arises, when cloud computing (uploading files and data onto the cloud) is applied. Cloud computing is extremely common these days since cloud computing is able to bring the advantage of storing less information on local machines. This means that more advanced experiments or analysis, for example machine learning, can be conducted by renting a server or virtual machine (VM). While there might be a contract between users and the company hosting the cloud that none of the data of the experiment should be shared with any other party, renting a server can implicitly leak your data to the company that owns the server/VM. Consequently, the absolute privacy is not guaranteed in this case. When the problem comes to big data (however useful it may be), yet another technical issue appears as big data is highly sensitive and it contains behavioural information about users. Sometimes, with only behavioural data and no additional data about the user, a user can be identified. We can try to fix the problem by simply removing the name of the users but privacy concerns still exist. When data is rather complicated and big, it includes a large number of attributes (name, age, sex, educational background, etc.) which can be represented as a n-column vector and n is quite large in this case. When n increases, the possibility of two vectors having exactly the same attributes on n dimensions decreases dramatically. Thus, an attacker is relatively easily able to identify individual users even if some fields such as “name” are removed. According to a contemporary science journal, for a large enough data set, as long as four points of an attribute, for example locations at four different times, is acknowledged, the possibility to uniquely identify the individual user is as high as 95%. Therefore, the high probability of correct identification comes at an extremely low cost for the attacker. When an attacker has certain access to part of the data, the user is even more vulnerable and the data becomes hard to anonymize. While a possible solution to this is to disable the direct access to the whole data set, only exposing aggregate data, and letting data to be accessed only by sending queries by authorized analysts, the attackers can still succeed by sending appropriate queries to launch an intersection attack. The attacker can retrieve the data by sending two queries such that the set of the aggregate data of the first query is completely contained in the set of data of the second query. Therefore, by correctly modifying the questions asked, the attacker can obtain the data of an individual user by calculating the difference between the two answers.
  <a href="#section02"style="color: black"><span></span>Scroll</a>
</section>
<section id="section02" class="demo">
  <div id="heading2">
    Different Solutions
  </div>
  <div id="text2">
    The basic solution to the simple attack model is using end to end encryption (E2EE). E2EE protects the data from being revealed in the process of communication. Basically, it encrypts data at the web level and decrypts it at the database or server. WhatsApp is an app which uses E2EE to protect users messages from attackers. Apparently, others in principle do not have access to the information in a private conversation. The second protective strategy is by employing Multiparty Computation (MPC). It only reveals aggregated information, but conceals individual information. For example, there are some people in the room, each of whom owns a secret bit of number, either 0 or 1. Instead of disclosing the choice of every individual, only the sum of the bits is made public. Another more advanced technique of encryption is homomorphic encryption. Equipped with all the capabilities of ordinary encryption, homomorphic encryption also allows operations between encrypted data. In other words, the order of decryption and performing operations is unimportant and will get the same result. In the process of encryption and decryption, no input or output is put at risk. Also, avoidance of decrypting every bit of information significantly reduces the cost of cryptography. The problem of cloud computing mentioned above can be solved perfectly by using homomorphic encryption. In the case of intersection attack, the solution is usually much more complicated. An ideally minimum amount of noise needs to be added to the data so that no individual data can be retrieved by only sending different queries. The amount of noise added to the data set should be precisely determined by sophisticated mathematical calculations such that no information of any individual user may be disclosed. Typically, alongside the high cost of such mathematical analysis, the data itself tends to become harder to use since usually a lot of noise need to be added to ensure absolute privacy. Meanwhile, the accuracy drops as noise increases. As a result, sometimes a balance or trade-off between privacy and utility is achieved according to certain limitations of privacy or accuracy. For example, we may need relatively high accuracy with error less than 5%. Thus, complete privacy might be sacrificed for a better experimental result. Differential privacy is just a realm that aims at finding a balance between privacy and utility. However, for most systems, the disparity between the current privacy-preserving model and the ideal one is still considerably large. It generally takes too much time to do even simple encryption when applying homomorphic encryption. Also, protocol or software development can also be another viable approach to ensure data privacy.
  </div>
  <a href="#section03"style="color: black"><span></span>Scroll</a>
</section>
<section id="section03" class="demo">
  <div id="heading3">
    Field of Research - Differential Privacy
  </div>
  <div id="text3">
    A method of protecting privacy is keeping the user data in a database which doesn't answer queries when the data is small enough for an attacker to learn anything at the individual level. The attacker, however, can make the queries in such a way that he can deduce the information he wants: for example, if he knows an individual’s data has some specific properties, he can first ask for information about all the users and then query for the users not satisfying the specific properties. The difference in these two queries will be the user’s data. Although this protection technique is flawed, the query-based systems could be a solution to the privacy problem. Differential privacy is a statistical method that adds noise to the user data, making it harder to learn about specific individuals while maintaining accuracy at the population level. The inventors of the technique won the Gödel Prize in 2017 (their paper was published in 2006). Differential privacy can be implemented in two ways: local or private. In local models, the users add noise to their data and send it to an aggregator, which, in turn, sends it to the data scientists. The accumulated noise makes it impractical to learn anything in many real life applications, requiring a larger dataset than would be required if the noise wasn’t added. In global models users give their data to an aggregator, where it is mixed with noise and sent to the data scientist. A first problem with this approach is that the aggregator can be corrupt, resulting in the users losing their privacy. Cryptography has shown, however, that it is possible to perform computations on encrypted data without knowing the decryption key. This means that the information can be encrypted by the users, processed by aggregators and then sent to the data scientists for decryption.
Differential privacy is a growing topic in computer science research (which can be seen in the number of citations of related academic papers), but it hasn’t reached an acceptable trade-off between the utility of the modified data and the users’ privacy.
  </div>
  <a href="#section04"style="color: black"><span></span>Scroll</a>
</section>
<section id="section04" class="demo">
  <div id="heading4">
    Pioneers of the Field
  </div>
  <div id="subheading41">
    Amazon
  </div>
  <div id="text41">
    Amazon wants to be a customer focused company which has led them to research new methods of keeping users data private and secure. They have recently produced a paper which investigates the correct amount of noise needed to be added to keep individual users’ data private while still allowing machine learning algorithms to learn frequent patterns in the data. In the paper they reveal a new method which allows a higher level of privacy with the same level of accuracy or a higher level of accuracy with a the same level of privacy.
  </div>
  <div id="subheading42">
    Apple
  </div>
  <div id="text42">
    To improve a user’s experience on a an Apple product, Apple will record data about how a user uses their product. The user however doesn’t want Apple to know exactly what they’re doing and the user is entitled to their privacy. This is why Apple has started researching and using differential privacy so they can record data about their users and then add noise to it so they mask the individual user’s data. On a large scale, Apple can analyse the accumulated data and learn how to improve users’ experiences but on a individual scale, Apple doesn’t know and can’t find out what their users are doing. They also limit the amount of data sent from an individual user so if someone tries to take an average of the data sent from one user, the noise will still have a prominent effect which means the users’ data will still be private.
  </div>
  <div id="subheading43">
    Google
  </div>
  <div id="text43">
    Google was one of the first companies to use differential privacy commercially. Google does a lot of research into machine learning, which requires a large amount of practice data and the data needs to be kept private when being used. To do this Google created their own system called RAPPOR (Randomized Aggregatable Privacy-Preserving Ordinal Response) which can collect statistics from users but doesn’t breach their individual privacy. Google Brain has also started to research differential privacy and have released some papers investigating it.
  </div>

  <a href="#thanks"style="color: black"><span></span>Scroll</a>
</section>

<section id="thanks">
  <div>
    <h2>Thanks!</h2>
    <p><a href="Home.html" target="_parent">&lt; Back To Article</a></p>
  </div>
</section>


    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script type="text/javascript">
      window.onscroll = function() {
        scrollEvent();
      }
      var sideNav = document.getElementById("sideNav");
      function scrollEvent() {
        var sticky = sideNav.offsetTop;
        if(window.pageYOffset >= sticky) {
          sideNav.classList.add("sticky");
        } else {
          sideNav.classList.remove("sticky");
        }
      }
    </script>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
    <script src="scrolldown.js"></script>
  </body>
</html>
